# GLAVOX - Sign Language Recognition System

GLAVOX is an advanced sign language recognition system that uses deep learning to convert sign language gestures into text and speech in real-time. The system is built using pure machine learning without relying on third-party services.

## ğŸš€ Features

- Real-time sign language gesture recognition
- Hand detection and tracking using MediaPipe
- Custom CNN model for gesture classification
- Custom TTS (Text-to-Speech) model for speech synthesis
- Real-time video processing and display
- Audio output for recognized gestures

## ğŸ’» Technologies Used

### Core Technologies
- **Python**: Primary programming language
- **PyTorch**: Deep learning framework
- **OpenCV**: Computer vision processing
- **MediaPipe**: Hand tracking and landmark detection
- **NumPy**: Numerical computations
- **SciPy**: Scientific computing
- **Librosa**: Audio processing

### Development Tools
- **Git**: Version control
- **VS Code**: IDE
- **Jupyter Notebook**: Model development and testing
- **Docker**: Containerization (optional)

### Cloud Services (Optional)
- **Google Colab**: For model training
- **AWS S3**: Model storage
- **Heroku**: Deployment

## ğŸ”„ External Resources & Outsources

### Datasets
1. **Sign Language Datasets**:
   - [ASL Alphabet Dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
   - [Sign Language MNIST](https://www.kaggle.com/datasets/danrasband/asl-alphabet-test)
   - [HandGestureDataset](https://www.kaggle.com/datasets/gti-upm/leapgestrecog)

2. **Audio Datasets**:
   - [LibriSpeech](https://www.openslr.org/12/)
   - [VCTK Corpus](https://datashare.ed.ac.uk/handle/10283/3443)

### Pre-trained Models
1. **Hand Detection**:
   - MediaPipe Hands
   - OpenCV DNN

2. **Feature Extraction**:
   - ResNet50 (optional)
   - VGG16 (optional)

### APIs & Services
1. **Development APIs**:
   - Google Cloud Vision API (optional)
   - AWS Rekognition (optional)

2. **Deployment Services**:
   - Heroku
   - AWS EC2
   - Google Cloud Platform

### Research Papers & References
1. **Sign Language Recognition**:
   - "Real-time Sign Language Recognition using Deep Learning"
   - "Hand Gesture Recognition using CNN"

2. **TTS Systems**:
   - "Tacotron: Towards End-to-End Speech Synthesis"
   - "FastSpeech: Fast, Robust and Controllable Text to Speech"

## ğŸ§  Machine Learning Architecture

### 1. Sign Language Recognition (CNN Model)
- **Architecture**: Custom Convolutional Neural Network
- **Input**: RGB video frames (224x224x3)
- **Layers**:
  - 3 Convolutional layers with ReLU activation
  - MaxPooling layers
  - Fully connected layers
  - Dropout for regularization
- **Output**: 26 classes (A-Z)

### 2. Text-to-Speech (TTS Model)
- **Architecture**: LSTM-based sequence-to-sequence model
- **Input**: Text features (256 dimensions)
- **Layers**:
  - LSTM Encoder
  - LSTM Decoder
  - Linear output layer
- **Output**: Audio features (80 dimensions)

## ğŸ“Š Data Processing Pipeline

1. **Video Input Processing**:
   - Frame capture from webcam
   - Resize to 224x224
   - Normalize pixel values

2. **Hand Detection**:
   - MediaPipe hand tracking
   - Landmark extraction
   - Hand region cropping

3. **Gesture Recognition**:
   - Feature extraction
   - CNN model inference
   - Gesture classification

4. **Speech Synthesis**:
   - Text feature generation
   - TTS model inference
   - Audio waveform generation

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (recommended)
- Webcam

### Step 1: Clone the Repository
```bash
git clone https://github.com/yourusername/glavox.git
cd glavox/GLAVOX_SERVER
```

### Step 2: Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Train Models
```bash
# Train CNN model
python train_cnn.py

# Train TTS model
python train_tts.py
```

### Step 5: Run the Application
```bash
python main.py
```

## ğŸ“ Project Structure

```
GLAVOX_SERVER/
â”œâ”€â”€ ml_models/
â”‚   â”œâ”€â”€ sign_language_cnn/
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â””â”€â”€ tts_model/
â”‚       â””â”€â”€ model.py
â”œâ”€â”€ video_processing/
â”‚   â”œâ”€â”€ hand_detection.py
â”‚   â””â”€â”€ gesture_recognition.py
â”œâ”€â”€ text_to_speech/
â”‚   â””â”€â”€ speech_synthesis.py
â”œâ”€â”€ main.py
â”œâ”€â”€ train_cnn.py
â”œâ”€â”€ train_tts.py
â””â”€â”€ requirements.txt
```

## ğŸ¯ Model Training

### CNN Model Training
- Uses PyTorch for training
- Implements custom data loading
- Supports GPU acceleration
- Includes validation metrics
- Saves model checkpoints

### TTS Model Training
- Sequence-to-sequence learning
- Custom loss functions
- Audio feature processing
- Model checkpointing

## ğŸ”§ Configuration

### Model Parameters
- CNN Model:
  - Input size: 224x224x3
  - Batch size: 32
  - Learning rate: 0.001
  - Epochs: 50

- TTS Model:
  - Input dimension: 256
  - Hidden dimension: 512
  - Output dimension: 80
  - Batch size: 32
  - Learning rate: 0.001

## ğŸ“ˆ Performance Metrics

### CNN Model
- Training accuracy: ~95%
- Validation accuracy: ~90%
- Inference time: <100ms

### TTS Model
- Training loss: <0.1
- Validation loss: <0.15
- Audio quality: 16kHz, 16-bit

## ğŸ” Usage

1. Start the application:
```bash
python main.py
```

2. Position your hand in front of the camera
3. Make sign language gestures
4. The system will:
   - Detect your hand
   - Recognize the gesture
   - Display the corresponding letter
   - Convert to speech

## ğŸ› ï¸ Development

### Adding New Gestures
1. Collect training data
2. Update the gesture mapping in `main.py`
3. Retrain the CNN model

### Customizing TTS
1. Modify the TTS model architecture
2. Update the audio processing pipeline
3. Retrain the TTS model

## ğŸ“š Dependencies

- PyTorch >= 1.9.0
- OpenCV >= 4.5.3
- MediaPipe >= 0.8.9
- NumPy >= 1.19.5
- SciPy >= 1.7.1
- Librosa >= 0.8.1

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- MediaPipe for hand tracking
- PyTorch team for the deep learning framework
- OpenCV for computer vision capabilities
- Kaggle for providing datasets
- Google Colab for training resources
- Research paper authors for their contributions

## ğŸ“ Support

For support, please open an issue in the GitHub repository or contact the maintainers.

## ğŸ”„ Updates

- Latest update: Added real-time hand tracking
- Improved gesture recognition accuracy
- Enhanced TTS quality
- Added support for multiple gestures
- Integrated with external datasets
- Added cloud deployment options
